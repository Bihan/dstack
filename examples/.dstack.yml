type: service
name: llama31

python: "3.11"
env:
  - HF_TOKEN
  - MODEL_ID=meta-llama/Llama-3.2-1B
  - MAX_MODEL_LEN=4096
commands:
  - pip install vllm
  - curl -o simple_chat_template.jinja https://github.com/Bihan/vllm/blob/main/examples/simple_chat_template.jinja
  - vllm serve $MODEL_ID
    --max-model-len $MAX_MODEL_LEN
    --chat-template simple_chat_template.jinja

auth: false

port: 8000
# Register the model
model:
  name: meta-llama/Llama-3.2-1B
  type: chat
  format: openai

# Uncomment to leverage spot instances
#spot_policy: auto

# Uncomment to cache downloaded models
#volumes:
#  - /root/.cache/huggingface/hub:/root/.cache/huggingface/hub

resources:
  gpu: 24GB
  # Uncomment if using multiple GPUs
  #shm_size: 24GB
