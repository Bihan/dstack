# https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html#framework-matrix-2024
type: dev-environment
# The name is optional, if not specified, generated randomly
name: trt-h200
image: nvcr.io/nvidia/tritonserver:25.01-trtllm-python-py3
# image: nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3

ide: vscode

spot_policy: on-demand

resources:
  gpu: 8:H200
  shm_size: 32GB

env:
  - HF_TOKEN
  - MODEL_REPO=https://huggingface.co/deepseek-ai/DeepSeek-R1
  - MAX_SEQ_LEN=4096
  - MAX_INPUT_LEN=512
  - MAX_BATCH_SIZE=4
  - SCRIPT_DIR=/workflow/TensorRT-LLM/examples/deepseek_v3
  - CHECK_POINT_DIR=/workflow/TensorRT-LLM/examples/deepseek_v3/ds-3.2-ckpt
  - ENGINE_DIR=/workflow/TensorRT-LLM/examples/deepseek_v3/ds-3.2-engine
  - MODEL_DOWNLOAD_DIR=/root/.cache/DeepSeek-R1
  - TOKENIZER_DIR=/root/.cache/DeepSeek-R1 # for Deepseek it will be DeepSeek-V2-Lite
  - MODEL_FOLDER=/workflow/triton_model_repo
  - TRITON_MAX_BATCH_SIZE=1
  - INSTANCE_COUNT=1
  - MAX_QUEUE_DELAY_MS=0
  - MAX_QUEUE_SIZE=0
  - FILL_TEMPLATE_SCRIPT=/workflow/tensorrtllm_backend/tools/fill_template.py
  - DECOUPLED_MODE=true # true for streaming

init:
  # - git clone --branch v0.17.0 --depth 1 https://github.com/triton-inference-server/tensorrtllm_backend.git
  # - git clone --branch v0.17.0 --single-branch https://github.com/NVIDIA/TensorRT-LLM.git
  - git clone --branch v0.17.0 --depth 1 https://github.com/triton-inference-server/tensorrtllm_backend.git
  - git clone --branch deepseek  --single-branch https://github.com/NVIDIA/TensorRT-LLM.git
  - git clone https://github.com/triton-inference-server/server.git
  - cd $SCRIPT_DIR
  - apt-get -y install git git-lfs
  - git lfs install
  - git config --global credential.helper store
  - huggingface-cli login --token $HF_TOKEN --add-to-git-credential
  # - git clone "$MODEL_REPO" "$MODEL_DOWNLOAD_DIR"
  # - python3 convert_checkpoint.py --model_dir $TOKENIZER_DIR  --output_dir $CHECK_POINT_DIR --dtype bfloat16 --tp_size $DSTACK_GPUS_NUM
  # - trtllm-build --checkpoint_dir $CHECK_POINT_DIR --gemm_plugin bfloat16 --output_dir $ENGINE_DIR --max_seq_len $MAX_SEQ_LEN --max_input_len $MAX_INPUT_LEN --max_batch_size $MAX_BATCH_SIZE --gpt_attention_plugin bfloat16 --use_paged_context_fmha enable
  # - mpirun --allow-run-as-root -n $DSTACK_GPUS_NUM python3 ../run.py --engine_dir $ENGINE_DIR  --max_output_len 40 --tokenizer_dir $TOKENIZER_DIR  --input_text "What is Deep Learning?"
  # - mkdir $MODEL_FOLDER
  # - cp -r /workflow/tensorrtllm_backend/all_models/inflight_batcher_llm/* $MODEL_FOLDER/
  # - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},logits_datatype:TYPE_BF16
  # - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}
  # - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_queue_size:${MAX_QUEUE_SIZE},encoder_input_features_data_type:TYPE_BF16,logits_datatype:TYPE_BF16
  # - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT},max_queue_size:${MAX_QUEUE_SIZE}
  # - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT},logits_datatype:TYPE_BF16
  # - python3 /workflow/tensorrtllm_backend/scripts/launch_triton_server.py --world_size=$DSTACK_GPUS_NUM --model_repo=${MODEL_FOLDER} --log-file "/workflow/triton_log.txt"
  
  # - python3 /workflow/server/python/openai/openai_frontend/main.py --model-repository ${MODEL_FOLDER} --tokenizer meta-llama/Llama-3.2-1B-Instruct --openai-port 8000


volumes:
  - /home/dstack/.cache/DeepSeek-R1:/root/.cache/DeepSeek-R1

  #SEND REQUEST TRITON INFERENCE SERVER
#   curl -X POST localhost:8000/v2/models/ensemble/generate -d \
# '{
# "text_input": "A farmer with a wolf, a goat, and a cabbage must cross a river by boat. The boat can carry only the farmer and a single item. If left unattended together, the wolf would eat the goat, or the goat would eat the cabbage. How can they cross the river without anything being eaten?",
# "parameters": {
# "max_tokens": 256,
# "bad_words":[""],
# "stop_words":[""]
# }
# }' | jq
