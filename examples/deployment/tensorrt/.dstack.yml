type: service
name: tensorrt-service

image: nvcr.io/nvidia/tritonserver:25.01-trtllm-python-py3

env:
  - HF_TOKEN
  - MODEL_REPO=https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
  - MAX_SEQ_LEN=4096
  - MAX_INPUT_LEN=512
  - MAX_BATCH_SIZE=4
  - SCRIPT_DIR=/workflow/TensorRT-LLM/examples/llama
  - CHECK_POINT_DIR=/workflow/TensorRT-LLM/examples/llama/llama-3.2-ckpt
  - ENGINE_DIR=/workflow/TensorRT-LLM/examples/llama/llama-3.2-engine
  - TOKENIZER_DIR=/workflow/TensorRT-LLM/examples/llama/Llama-3.2-1B-Instruct # for Deepseek it will be DeepSeek-V2-Lite
  - MODEL_FOLDER=/workflow/triton_model_repo
  - TRITON_MAX_BATCH_SIZE=1
  - INSTANCE_COUNT=1
  - MAX_QUEUE_DELAY_MS=0
  - MAX_QUEUE_SIZE=0
  - FILL_TEMPLATE_SCRIPT=/workflow/tensorrtllm_backend/tools/fill_template.py
  - DECOUPLED_MODE=true # true for streaming

commands:
  - git clone --branch v0.17.0 --depth 1 https://github.com/triton-inference-server/tensorrtllm_backend.git
  - git clone --branch v0.17.0 --single-branch https://github.com/NVIDIA/TensorRT-LLM.git
  - git clone https://github.com/triton-inference-server/server.git
  - cd $SCRIPT_DIR
  - apt-get -y install git git-lfs
  - git lfs install
  - git config --global credential.helper store
  - huggingface-cli login --token $HF_TOKEN --add-to-git-credential
  - git clone $MODEL_REPO
  - python3 convert_checkpoint.py --model_dir $TOKENIZER_DIR  --output_dir $CHECK_POINT_DIR --dtype bfloat16 --tp_size $DSTACK_GPUS_NUM
  - trtllm-build --checkpoint_dir $CHECK_POINT_DIR --gemm_plugin bfloat16 --output_dir $ENGINE_DIR --max_seq_len $MAX_SEQ_LEN --max_input_len $MAX_INPUT_LEN --max_batch_size $MAX_BATCH_SIZE --gpt_attention_plugin bfloat16 --use_paged_context_fmha enable
  # - python3 ../run.py --engine_dir $ENGINE_DIR  --max_output_len 40 --tokenizer_dir $TOKENIZER_DIR  --input_text "What is Deep Learning?"
  - mkdir $MODEL_FOLDER
  - cp -r /workflow/tensorrtllm_backend/all_models/inflight_batcher_llm/* $MODEL_FOLDER/
  - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},logits_datatype:TYPE_BF16
  - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}
  - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_queue_size:${MAX_QUEUE_SIZE},encoder_input_features_data_type:TYPE_BF16,logits_datatype:TYPE_BF16
  - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT},max_queue_size:${MAX_QUEUE_SIZE}
  - python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT},logits_datatype:TYPE_BF16
  # - python3 /workflow/tensorrtllm_backend/scripts/launch_triton_server.py --world_size=1 --model_repo=${MODEL_FOLDER} --log-file "/workflow/triton_log.txt"
  - python3 /workflow/server/python/openai/openai_frontend/main.py --model-repository ${MODEL_FOLDER} --tokenizer meta-llama/Llama-3.2-1B-Instruct --openai-port 8000


port: 8000

model: ensemble

resources:
  gpu: A100:40GB
