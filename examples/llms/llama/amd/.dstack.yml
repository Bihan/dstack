type: dev-environment
name: llama4-scout-serve

image: rocm/vllm-dev:llama4-20250405

ide: vscode

env:
  - HF_TOKEN
  - MODEL_ID=meta-llama/Llama-4-Scout-17B-16E-Instruct
  - VLLM_WORKER_MULTIPROC_METHOD=spawn
  - VLLM_USE_MODELSCOPE=False
  - VLLM_USE_TRITON_FLASH_ATTN=0
#  - MAX_MODEL_LEN=4096
#  - MAX_NUM_SEQS=8
init:
  - vllm serve $MODEL_ID --disable-log-requests -tp 2 --max-num-seqs 16 --no-enable-prefix-caching --max_num_batched_tokens=8000 --max_model_len 8000

#port: 8000
# Register the model
#model: meta-llama/Llama-3.2-11B-Vision-Instruct

# Uncomment to cache downloaded models
#volumes:
#  - /root/.cache/huggingface/hub:/root/.cache/huggingface/hub

resources:
  gpu: Mi300x:2
