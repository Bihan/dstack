type: task
name: agent-fine-tuning
nodes: 2
image: whatcanyousee/verl:ngc-cu124-vllm0.8.5-sglang0.4.6-mcore0.12.0-te2.2

env:
- WANDB_API_KEY

commands:
  - wget -O miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
  - bash miniconda.sh -b -p /workflow/miniconda
  - eval "$(/workflow/miniconda/bin/conda shell.bash hook)"
  - git clone https://github.com/RAGEN-AI/RAGEN.git
  - cd RAGEN
  - bash scripts/setup_ragen.sh
  - conda activate ragen
  - cd verl
  - pip install --no-deps -e .
  - pip install hf_transfer hf_xet
  - pip uninstall -y ray
  - pip install -U "ray[default]"
  - >
    if [ $DSTACK_NODE_RANK = 0 ]; then 
        ray start --head --port=6379;
    else
        ray start --address=$DSTACK_MASTER_NODE_IP:6379
    fi
ports:
  - 8265 # ray dashboard port
resources:
  gpu: nvidia:8:80GB 
  shm_size: 128GB

volumes:
  - /checkpoints:/checkpoints

# RAY_ADDRESS='http://localhost:8265' \
# ray job submit \
# -- bash -c "\
#   export PYTHONPATH=/workflow/RAGEN; \
#   cd /workflow/RAGEN; \
#   /workflow/miniconda/envs/ragen/bin/python train.py \
#     --config-name base \
#     system.CUDA_VISIBLE_DEVICES=[0,1,2,3,4,5,6,7] \
#     model_path=Qwen/Qwen2.5-7B-Instruct \
#     es_manager.train.env_configs.tags='[SimpleSokoban,FrozenLake,Bandit]' \
#     trainer.experiment_name=agent-fine-tuning-Qwen2.5-7B \
#     trainer.n_gpus_per_node=8 \
#     trainer.nnodes=2 \
#     micro_batch_size_per_gpu=2 \
#     trainer.default_local_dir=/checkpoints \
#     trainer.save_freq=50 \
#     actor_rollout_ref.rollout.tp_size_check=False \
#     actor_rollout_ref.rollout.tensor_model_parallel_size=4"


# RAY_ADDRESS='http://localhost:8265' \
# ray job submit \
# -- bash -c "\
#   export PYTHONPATH=/workflow/RAGEN; \
#   cd /workflow/RAGEN; \
#   /workflow/miniconda/envs/ragen/bin/python train.py \
#     --config-name base \
#     system.CUDA_VISIBLE_DEVICES=[0,1,2,3,4,5,6,7] \
#     model_path=Qwen/Qwen2.5-7B-Instruct \
#     trainer.experiment_name=agent-fine-tuning-Qwen2.5-7B \
#     trainer.n_gpus_per_node=8 \
#     trainer.nnodes=2 \
#     micro_batch_size_per_gpu=2 \
#     trainer.default_local_dir=/checkpoints \
#     trainer.save_freq=50 \
#     actor_rollout_ref.rollout.tp_size_check=False \
#     actor_rollout_ref.rollout.tensor_model_parallel_size=4"

# RAY_ADDRESS='http://localhost:8265' \
# ray job submit \
# -- bash -c "\
#   export PYTHONPATH=/workflow/RAGEN; \
#   cd /workflow/RAGEN; \
#   /workflow/miniconda/envs/ragen/bin/python train.py \
#     --config-name base \
#     system.CUDA_VISIBLE_DEVICES=[0,1,2,3,4,5,6,7] \
#     model_path=Qwen/Qwen2.5-7B-Instruct \
#     trainer.experiment_name=agent-fine-tuning-Qwen2.5-7B \
#     trainer.n_gpus_per_node=8 \
#     trainer.nnodes=2 \
#     ppo_mini_batch_size=64 \
#     es_manager.train.env_groups=16 \
#     es_manager.train.env_configs.n_groups=[16] \
#     trainer.default_local_dir=/checkpoints \
#     trainer.save_freq=50 \
#     actor_rollout_ref.rollout.tp_size_check=False \
#     actor_rollout_ref.rollout.tensor_model_parallel_size=4"

# LORA: Just set lora.rank
# RAY_ADDRESS='http://localhost:8265' \
# ray job submit \
# -- bash -c "\
#   export PYTHONPATH=/workflow/RAGEN; \
#   cd /workflow/RAGEN; \
#   /workflow/miniconda/envs/ragen/bin/python train.py \
#     --config-name base.yaml \
#     system.CUDA_VISIBLE_DEVICES=[0,1,2,3,4,5,6,7] \
#     model_path=Qwen/Qwen2.5-7B-Instruct \
#     lora.rank=8 \
#     trainer.n_gpus_per_node=8 \
#     trainer.nnodes=2 \
#     ppo_mini_batch_size=64 \
#     es_manager.train.env_groups=16 \
#     es_manager.train.env_configs.n_groups=[16] \
#     trainer.default_local_dir=/checkpoints \
#     trainer.save_freq=50 \
#     actor_rollout_ref.rollout.tp_size_check=False \
#     actor_rollout_ref.rollout.tensor_model_parallel_size=4"
